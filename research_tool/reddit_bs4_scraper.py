"""
Reddit Web Scraper using BeautifulSoup
ë¯¼ì¤€ í˜ë¥´ì†Œë‚˜: Python í•™ìŠµ ì¢Œì ˆ, ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì§„ì… ì¥ë²½ ë¶„ì„
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import re
from tqdm import tqdm
import os

class RedditScraper:
    def __init__(self):
        self.posts = []
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def scrape_subreddit_search(self, subreddit, search_query, max_posts=50):
        """
        ì„œë¸Œë ˆë”§ ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§

        Args:
            subreddit: ì„œë¸Œë ˆë”§ ì´ë¦„ (ì˜ˆ: 'learnpython')
            search_query: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_posts: ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜
        """
        print(f"\nğŸ” r/{subreddit} ê²€ìƒ‰: '{search_query}'")

        # Old Reddit ì‚¬ìš© (ì •ì  HTML)
        search_url = f"https://old.reddit.com/r/{subreddit}/search/?q={search_query.replace(' ', '+')}&restrict_sr=1&sort=relevance&t=year&limit=100"

        try:
            response = self.session.get(search_url, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # ê²Œì‹œê¸€ ì¶”ì¶œ
            post_elements = soup.find_all('div', class_='thing', attrs={'data-type': 'link'})

            posts_collected = 0
            for post_elem in post_elements[:max_posts]:
                try:
                    post_data = self._extract_post_info(post_elem, subreddit, search_query)
                    if post_data:
                        self.posts.append(post_data)
                        posts_collected += 1
                except Exception as e:
                    continue

            print(f"  âœ“ {posts_collected}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")

        except requests.exceptions.RequestException as e:
            print(f"  âŒ ì˜¤ë¥˜: {str(e)}")
        except Exception as e:
            print(f"  âŒ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")

    def _extract_post_info(self, post_element, subreddit, search_query):
        """ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì œëª©
            title_elem = post_element.find('a', class_='title')
            title = title_elem.get_text(strip=True) if title_elem else "N/A"

            # URL
            url = title_elem['href'] if title_elem and 'href' in title_elem.attrs else "N/A"
            if url and not url.startswith('http'):
                url = f"https://old.reddit.com{url}"

            # ì‘ì„±ì
            author_elem = post_element.find('a', class_='author')
            author = author_elem.get_text(strip=True) if author_elem else "[deleted]"

            # ì ìˆ˜ (upvotes)
            score_elem = post_element.find('div', class_='score unvoted')
            score = score_elem.get('title', '0') if score_elem else "0"

            # ëŒ“ê¸€ ìˆ˜
            comments_elem = post_element.find('a', class_='comments')
            comments_text = comments_elem.get_text(strip=True) if comments_elem else "0 comments"
            num_comments = re.search(r'(\d+)', comments_text)
            num_comments = num_comments.group(1) if num_comments else "0"

            # ì‘ì„± ì‹œê°„
            time_elem = post_element.find('time')
            created_time = time_elem.get('title', 'N/A') if time_elem else "N/A"

            # ë„ë©”ì¸
            domain_elem = post_element.find('span', class_='domain')
            domain = domain_elem.get_text(strip=True) if domain_elem else ""

            return {
                'subreddit': f"r/{subreddit}",
                'search_query': search_query,
                'title': title,
                'author': author,
                'score': score,
                'num_comments': num_comments,
                'created_at': created_time,
                'domain': domain,
                'url': url,
                'collected_at': datetime.now()
            }

        except Exception as e:
            return None

    def scrape_multiple_queries(self, subreddit_queries, max_posts_per_query=50):
        """
        ì—¬ëŸ¬ ì„œë¸Œë ˆë”§/ê²€ìƒ‰ì–´ ì¡°í•©ìœ¼ë¡œ í¬ë¡¤ë§

        Args:
            subreddit_queries: {subreddit: [keywords]} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*60}")
        print(f"ğŸ’¬ Reddit ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        print(f"{'='*60}\n")

        total_queries = sum(len(keywords) for keywords in subreddit_queries.values())
        current = 0

        for subreddit, keywords in subreddit_queries.items():
            print(f"\n{'â”€'*60}")
            print(f"ğŸ“Š r/{subreddit} - {len(keywords)}ê°œ í‚¤ì›Œë“œ")
            print(f"{'â”€'*60}")

            for keyword in keywords:
                current += 1
                print(f"[{current}/{total_queries}]", end=" ")
                self.scrape_subreddit_search(subreddit, keyword, max_posts_per_query)
                time.sleep(2)  # Rate limiting

        print(f"\n{'='*60}")
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(self.posts)}ê°œ ê²Œì‹œê¸€")
        print(f"{'='*60}\n")

    def to_dataframe(self):
        """DataFrameìœ¼ë¡œ ë³€í™˜"""
        if not self.posts:
            return pd.DataFrame()

        df = pd.DataFrame(self.posts)

        # ì¤‘ë³µ ì œê±°
        original_count = len(df)
        df = df.drop_duplicates(subset=['url'])
        removed = original_count - len(df)

        print(f"ğŸ“Š ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {len(df)}ê°œ ê³ ìœ  ê²Œì‹œê¸€ (ì¤‘ë³µ {removed}ê°œ ì œê±°)")
        return df

    def save_data(self, filename_prefix='reddit_posts'):
        """ë°ì´í„° ì €ì¥"""
        df = self.to_dataframe()

        if df.empty:
            print("âš  ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None, None

        os.makedirs('output', exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # CSV ì €ì¥
        csv_path = f"output/{filename_prefix}_{timestamp}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')

        # Excel ì €ì¥
        excel_path = f"output/{filename_prefix}_{timestamp}.xlsx"
        df.to_excel(excel_path, index=False, engine='openpyxl')

        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ:")
        print(f"  - CSV: {csv_path}")
        print(f"  - Excel: {excel_path}")

        return csv_path, excel_path, df


def main():
    """ì‹¤í–‰ ì˜ˆì‹œ"""
    # ë¯¼ì¤€ í˜ë¥´ì†Œë‚˜ íƒ€ê²Ÿ ì„œë¸Œë ˆë”§ ë° í‚¤ì›Œë“œ
    subreddit_queries = {
        'learnpython': [
            'give up',
            'too hard',
            'frustrated',
            'struggling',
            'quit python',
            'overwhelmed',
            'beginner'
        ],
        'learnprogramming': [
            'give up',
            'too hard',
            'overwhelmed',
            'losing motivation',
            'quit programming'
        ],
        'datascience': [
            'beginner',
            'getting started',
            'too expensive',
            'coursera',
            'udemy',
            'free resources',
            'learning path'
        ]
    }

    scraper = RedditScraper()
    scraper.scrape_multiple_queries(subreddit_queries, max_posts_per_query=30)

    # ë°ì´í„° ì €ì¥
    csv_path, excel_path, df = scraper.save_data('minjun_reddit_posts')

    if df is not None and not df.empty:
        print(f"\nğŸ“ˆ ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½:")
        print(f"  - ì´ ê²Œì‹œê¸€: {len(df)}ê°œ")
        print(f"  - ì„œë¸Œë ˆë”§ë³„ ë¶„í¬:")
        print(df.groupby('subreddit').size().to_string())

    return df


if __name__ == "__main__":
    main()
