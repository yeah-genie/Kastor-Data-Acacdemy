"""
Reddit Web Scraper using Playwright
ë¯¼ì¤€ í˜ë¥´ì†Œë‚˜: Python í•™ìŠµ ì¢Œì ˆ, ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì§„ì… ì¥ë²½ ë¶„ì„
"""

import asyncio
import json
import pandas as pd
from datetime import datetime
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout
from tqdm import tqdm
import time
import re

class RedditWebScraper:
    def __init__(self, headless=True):
        self.headless = headless
        self.posts = []

    async def scrape_subreddit_search(self, subreddit, search_query, max_posts=30):
        """
        ì„œë¸Œë ˆë”§ ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§

        Args:
            subreddit: ì„œë¸Œë ˆë”§ ì´ë¦„ (ì˜ˆ: 'learnpython')
            search_query: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_posts: ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜
        """
        async with async_playwright() as p:
            print(f"\nğŸ” r/{subreddit} ê²€ìƒ‰: '{search_query}'")

            browser = await p.chromium.launch(headless=self.headless)
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            page = await context.new_page()

            try:
                # Old Reddit ì‚¬ìš© (ë” ê°„ë‹¨í•œ HTML êµ¬ì¡°)
                search_url = f"https://old.reddit.com/r/{subreddit}/search/?q={search_query.replace(' ', '+')}&restrict_sr=1&sort=relevance&t=year"
                print(f"  ğŸ“„ URL: {search_url}")

                await page.goto(search_url, wait_until='domcontentloaded', timeout=30000)
                await asyncio.sleep(2)

                # ê²Œì‹œê¸€ ì¶”ì¶œ
                posts_collected = 0
                scroll_attempts = 0
                max_scroll_attempts = 5

                while posts_collected < max_posts and scroll_attempts < max_scroll_attempts:
                    # ê²Œì‹œê¸€ ì„ íƒì
                    post_elements = await page.query_selector_all('.thing[data-type="link"]')

                    for post_elem in post_elements[posts_collected:]:
                        if posts_collected >= max_posts:
                            break

                        try:
                            post_data = await self._extract_post_info(post_elem, subreddit, search_query)
                            if post_data:
                                self.posts.append(post_data)
                                posts_collected += 1
                        except Exception as e:
                            # print(f"    ê²Œì‹œê¸€ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
                            continue

                    # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë¡œë“œ
                    await page.evaluate('window.scrollBy(0, window.innerHeight * 2)')
                    await asyncio.sleep(1)
                    scroll_attempts += 1

                print(f"  âœ“ {posts_collected}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")

            except PlaywrightTimeout:
                print("  âš  í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼")
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {str(e)}")
            finally:
                await browser.close()

    async def _extract_post_info(self, post_element, subreddit, search_query):
        """ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì œëª©
            title_elem = await post_element.query_selector('.title a.title')
            title = await title_elem.inner_text() if title_elem else "N/A"

            # URL
            url = await title_elem.get_attribute('href') if title_elem else "N/A"
            if url and not url.startswith('http'):
                url = f"https://old.reddit.com{url}"

            # ì‘ì„±ì
            author_elem = await post_element.query_selector('.author')
            author = await author_elem.inner_text() if author_elem else "[deleted]"

            # ì ìˆ˜ (upvotes)
            score_elem = await post_element.query_selector('.score.unvoted')
            score_text = await score_elem.get_attribute('title') if score_elem else "0"
            score = score_text if score_text else "0"

            # ëŒ“ê¸€ ìˆ˜
            comments_elem = await post_element.query_selector('.comments')
            comments_text = await comments_elem.inner_text() if comments_elem else "0 comments"
            num_comments = re.search(r'(\d+)', comments_text)
            num_comments = num_comments.group(1) if num_comments else "0"

            # ì‘ì„± ì‹œê°„
            time_elem = await post_element.query_selector('time')
            created_time = await time_elem.get_attribute('title') if time_elem else "N/A"

            # ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸° (ìˆëŠ” ê²½ìš°)
            preview_elem = await post_element.query_selector('.expando')
            preview = await preview_elem.inner_text() if preview_elem else ""

            return {
                'subreddit': f"r/{subreddit}",
                'search_query': search_query,
                'title': title.strip(),
                'author': author.strip(),
                'score': score,
                'num_comments': num_comments,
                'created_at': created_time,
                'url': url,
                'preview': preview[:200] if preview else "",  # ì²« 200ì
                'collected_at': datetime.now()
            }

        except Exception as e:
            # print(f"      ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None

    async def scrape_multiple_queries(self, subreddit_queries, max_posts_per_query=30):
        """
        ì—¬ëŸ¬ ì„œë¸Œë ˆë”§/ê²€ìƒ‰ì–´ ì¡°í•©ìœ¼ë¡œ í¬ë¡¤ë§

        Args:
            subreddit_queries: {subreddit: [keywords]} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*60}")
        print(f"ğŸ’¬ Reddit ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        print(f"{'='*60}\n")

        total_queries = sum(len(keywords) for keywords in subreddit_queries.values())
        current = 0

        for subreddit, keywords in subreddit_queries.items():
            print(f"\n{'â”€'*60}")
            print(f"ğŸ“Š r/{subreddit} - {len(keywords)}ê°œ í‚¤ì›Œë“œ")
            print(f"{'â”€'*60}")

            for keyword in keywords:
                current += 1
                print(f"\n[{current}/{total_queries}]", end=" ")
                await self.scrape_subreddit_search(subreddit, keyword, max_posts_per_query)
                await asyncio.sleep(2)  # Rate limiting

        print(f"\n{'='*60}")
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(self.posts)}ê°œ ê²Œì‹œê¸€")
        print(f"{'='*60}\n")

    def to_dataframe(self):
        """DataFrameìœ¼ë¡œ ë³€í™˜"""
        if not self.posts:
            return pd.DataFrame()

        df = pd.DataFrame(self.posts)

        # ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset=['url'])

        print(f"ğŸ“Š ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {len(df)}ê°œ ê³ ìœ  ê²Œì‹œê¸€")
        return df

    def save_data(self, filename_prefix='reddit_posts'):
        """ë°ì´í„° ì €ì¥"""
        df = self.to_dataframe()

        if df.empty:
            print("âš  ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        import os
        os.makedirs('output', exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # CSV ì €ì¥
        csv_path = f"output/{filename_prefix}_{timestamp}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')

        # Excel ì €ì¥
        excel_path = f"output/{filename_prefix}_{timestamp}.xlsx"
        df.to_excel(excel_path, index=False, engine='openpyxl')

        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ:")
        print(f"  - CSV: {csv_path}")
        print(f"  - Excel: {excel_path}")

        return csv_path, excel_path, df


async def main():
    """ì‹¤í–‰ ì˜ˆì‹œ"""
    # ë¯¼ì¤€ í˜ë¥´ì†Œë‚˜ íƒ€ê²Ÿ ì„œë¸Œë ˆë”§ ë° í‚¤ì›Œë“œ
    subreddit_queries = {
        'learnpython': [
            'give up',
            'too hard',
            'frustrated',
            'beginner struggling',
            'quit python'
        ],
        'learnprogramming': [
            'give up',
            'too hard',
            'overwhelmed',
            'losing motivation'
        ],
        'datascience': [
            'beginner',
            'getting started',
            'too expensive',
            'coursera worth it',
            'free resources'
        ]
    }

    scraper = RedditWebScraper(headless=True)
    await scraper.scrape_multiple_queries(subreddit_queries, max_posts_per_query=20)

    # ë°ì´í„° ì €ì¥
    scraper.save_data('minjun_reddit_posts')


if __name__ == "__main__":
    asyncio.run(main())
