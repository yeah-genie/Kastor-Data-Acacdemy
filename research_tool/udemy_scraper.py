"""
Udemy Course Scraper using Playwright
ë¯¼ì¤€ í˜ë¥´ì†Œë‚˜ íƒ€ê²Ÿ: Python/ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì´ˆë³´ì ê°•ì˜ ë¶„ì„
"""

import asyncio
import json
import pandas as pd
from datetime import datetime
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout
from tqdm import tqdm
import time

class UdemyScraper:
    def __init__(self, headless=True):
        self.headless = headless
        self.courses = []

    async def scrape_search_results(self, search_query, max_pages=3):
        """
        Udemy ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§

        Args:
            search_query: ê²€ìƒ‰ í‚¤ì›Œë“œ (ì˜ˆ: 'python for beginners')
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        """
        async with async_playwright() as p:
            print(f"\nğŸ” ê²€ìƒ‰ ì¤‘: '{search_query}'")

            # Launch browser
            browser = await p.chromium.launch(headless=self.headless)
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            page = await context.new_page()

            try:
                # Udemy ê²€ìƒ‰ URL
                search_url = f"https://www.udemy.com/courses/search/?q={search_query.replace(' ', '+')}&sort=popularity"
                print(f"ğŸ“„ URL: {search_url}")

                await page.goto(search_url, wait_until='networkidle', timeout=30000)
                await asyncio.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

                for page_num in range(1, max_pages + 1):
                    print(f"\nğŸ“– í˜ì´ì§€ {page_num}/{max_pages} í¬ë¡¤ë§ ì¤‘...")

                    # ê°•ì˜ ì¹´ë“œ ì°¾ê¸°
                    await page.wait_for_selector('[data-purpose="course-card"]', timeout=10000)

                    # ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ê°•ì˜ ë¡œë“œ
                    for _ in range(3):
                        await page.evaluate('window.scrollBy(0, window.innerHeight)')
                        await asyncio.sleep(0.5)

                    # ê°•ì˜ ì •ë³´ ì¶”ì¶œ
                    courses = await page.query_selector_all('[data-purpose="course-card"]')

                    for course in courses:
                        try:
                            course_data = await self._extract_course_info(course, search_query, page)
                            if course_data:
                                self.courses.append(course_data)
                        except Exception as e:
                            print(f"  âš  ê°•ì˜ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
                            continue

                    print(f"  âœ“ {len(courses)}ê°œ ê°•ì˜ ì¶”ì¶œ ì™„ë£Œ")

                    # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                    if page_num < max_pages:
                        try:
                            next_button = await page.query_selector('[aria-label="ë‹¤ìŒ í˜ì´ì§€"], [aria-label="Next"]')
                            if next_button:
                                await next_button.click()
                                await asyncio.sleep(2)
                            else:
                                print("  â„¹ ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ")
                                break
                        except:
                            print("  â„¹ ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨")
                            break

            except PlaywrightTimeout:
                print("âš  í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼")
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            finally:
                await browser.close()

    async def _extract_course_info(self, course_element, search_query, page):
        """ê°•ì˜ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ê°•ì˜ ì œëª©
            title_elem = await course_element.query_selector('[data-purpose="course-title-url"] h3, [data-purpose="course-title"]')
            title = await title_elem.inner_text() if title_elem else "N/A"

            # ê°•ì˜ URL
            url_elem = await course_element.query_selector('a[href*="/course/"]')
            url = await url_elem.get_attribute('href') if url_elem else "N/A"
            if url and not url.startswith('http'):
                url = f"https://www.udemy.com{url}"

            # ê°•ì‚¬ëª…
            instructor_elem = await course_element.query_selector('[data-purpose="safely-set-inner-html:course-card:visible-instructors"]')
            instructor = await instructor_elem.inner_text() if instructor_elem else "N/A"

            # í‰ì 
            rating_elem = await course_element.query_selector('[data-purpose="rating-number"]')
            rating = await rating_elem.inner_text() if rating_elem else "N/A"

            # ë¦¬ë·° ìˆ˜
            reviews_elem = await course_element.query_selector('[data-purpose="reviews-count-text"]')
            reviews_text = await reviews_elem.inner_text() if reviews_elem else "0"
            reviews = reviews_text.replace('(', '').replace(')', '').replace(',', '').strip()

            # ìˆ˜ê°•ìƒ ìˆ˜
            students_elem = await course_element.query_selector('.course-card--student-count--1wT0t')
            students_text = await students_elem.inner_text() if students_elem else "0 students"

            # ê°€ê²©
            price_elem = await course_element.query_selector('[data-purpose="course-price-text"] span:last-child, .price-text--price-part--2-Nn0 span:last-child')
            price = await price_elem.inner_text() if price_elem else "N/A"

            # ë ˆë²¨ (ì´ˆê¸‰, ì¤‘ê¸‰, ê³ ê¸‰)
            level_elem = await course_element.query_selector('[data-purpose="course-level"]')
            level = await level_elem.inner_text() if level_elem else "N/A"

            # ê°•ì˜ ê¸¸ì´
            duration_elem = await course_element.query_selector('[data-purpose="course-content-length"]')
            duration = await duration_elem.inner_text() if duration_elem else "N/A"

            return {
                'search_query': search_query,
                'title': title.strip(),
                'instructor': instructor.strip(),
                'rating': rating.strip(),
                'num_reviews': reviews,
                'num_students': students_text.strip(),
                'price': price.strip(),
                'level': level.strip(),
                'duration': duration.strip(),
                'url': url,
                'collected_at': datetime.now()
            }

        except Exception as e:
            # print(f"    ê°•ì˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None

    async def scrape_multiple_queries(self, search_queries, max_pages=3):
        """ì—¬ëŸ¬ ê²€ìƒ‰ì–´ë¡œ í¬ë¡¤ë§"""
        print(f"\n{'='*60}")
        print(f"ğŸ“ Udemy ê°•ì˜ í¬ë¡¤ë§ ì‹œì‘")
        print(f"{'='*60}")
        print(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {len(search_queries)}ê°œ")
        print(f"í˜ì´ì§€ë‹¹ ìµœëŒ€: {max_pages}í˜ì´ì§€\n")

        for query in search_queries:
            await self.scrape_search_results(query, max_pages)
            await asyncio.sleep(2)  # Rate limiting

        print(f"\n{'='*60}")
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(self.courses)}ê°œ ê°•ì˜")
        print(f"{'='*60}\n")

    def to_dataframe(self):
        """DataFrameìœ¼ë¡œ ë³€í™˜"""
        if not self.courses:
            return pd.DataFrame()

        df = pd.DataFrame(self.courses)

        # ì¤‘ë³µ ì œê±° (ê°™ì€ ê°•ì˜ê°€ ì—¬ëŸ¬ ê²€ìƒ‰ì–´ì—ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)
        df = df.drop_duplicates(subset=['url'])

        print(f"ğŸ“Š ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {len(df)}ê°œ ê³ ìœ  ê°•ì˜")
        return df

    def save_data(self, filename_prefix='udemy_courses'):
        """ë°ì´í„° ì €ì¥"""
        df = self.to_dataframe()

        if df.empty:
            print("âš  ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        import os
        os.makedirs('output', exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # CSV ì €ì¥
        csv_path = f"output/{filename_prefix}_{timestamp}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')

        # Excel ì €ì¥
        excel_path = f"output/{filename_prefix}_{timestamp}.xlsx"
        df.to_excel(excel_path, index=False, engine='openpyxl')

        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ:")
        print(f"  - CSV: {csv_path}")
        print(f"  - Excel: {excel_path}")

        return csv_path, excel_path, df


async def main():
    """ì‹¤í–‰ ì˜ˆì‹œ"""
    # ë¯¼ì¤€ í˜ë¥´ì†Œë‚˜ íƒ€ê²Ÿ ê²€ìƒ‰ì–´
    search_queries = [
        'python for beginners',
        'python data science',
        'data science for beginners',
        'learn python programming',
        'data analysis python',
        'python pandas tutorial'
    ]

    scraper = UdemyScraper(headless=True)
    await scraper.scrape_multiple_queries(search_queries, max_pages=2)

    # ë°ì´í„° ì €ì¥
    scraper.save_data('minjun_udemy_courses')


if __name__ == "__main__":
    asyncio.run(main())
